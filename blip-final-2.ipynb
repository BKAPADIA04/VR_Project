{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11657781,"sourceType":"datasetVersion","datasetId":7315783},{"sourceId":11663595,"sourceType":"datasetVersion","datasetId":7319871},{"sourceId":11780463,"sourceType":"datasetVersion","datasetId":7396008},{"sourceId":11813676,"sourceType":"datasetVersion","datasetId":7420042},{"sourceId":11818079,"sourceType":"datasetVersion","datasetId":7423091}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T06:16:05.701013Z","iopub.execute_input":"2025-05-17T06:16:05.701595Z","iopub.status.idle":"2025-05-17T06:16:05.705928Z","shell.execute_reply.started":"2025-05-17T06:16:05.701568Z","shell.execute_reply":"2025-05-17T06:16:05.705015Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"final_df = pd.read_csv('/kaggle/input/finale-dataset/train_dataset.csv')\ntest_df = pd.read_csv('/kaggle/input/finale-dataset/test_dataset.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T06:19:31.631902Z","iopub.execute_input":"2025-05-17T06:19:31.632213Z","iopub.status.idle":"2025-05-17T06:19:31.753256Z","shell.execute_reply.started":"2025-05-17T06:19:31.632191Z","shell.execute_reply":"2025-05-17T06:19:31.752631Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"final_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T06:19:31.962474Z","iopub.execute_input":"2025-05-17T06:19:31.962713Z","iopub.status.idle":"2025-05-17T06:19:31.973111Z","shell.execute_reply.started":"2025-05-17T06:19:31.962694Z","shell.execute_reply":"2025-05-17T06:19:31.972490Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"      image_id                              question        answer  \\\n0  71BN3iMoGkL    What are the figurines sitting on?         Bench   \n1  71AH7yyOgCL  What brand is the mobile phone case?        Solimo   \n2  71ag2dwtVfL                   What brand is this?  AmazonBasics   \n3  61yqMXIGHVL  What lining do these envelopes have?        Bubble   \n4  71d57C76BpL              What is the shade shape?          Drum   \n\n              path  \n0  9d/9dd2d3f1.jpg  \n1  f3/f3ab1fbf.jpg  \n2  62/62075c07.jpg  \n3  2d/2d483c93.jpg  \n4  b8/b8b510a2.jpg  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>question</th>\n      <th>answer</th>\n      <th>path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>71BN3iMoGkL</td>\n      <td>What are the figurines sitting on?</td>\n      <td>Bench</td>\n      <td>9d/9dd2d3f1.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>71AH7yyOgCL</td>\n      <td>What brand is the mobile phone case?</td>\n      <td>Solimo</td>\n      <td>f3/f3ab1fbf.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>71ag2dwtVfL</td>\n      <td>What brand is this?</td>\n      <td>AmazonBasics</td>\n      <td>62/62075c07.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>61yqMXIGHVL</td>\n      <td>What lining do these envelopes have?</td>\n      <td>Bubble</td>\n      <td>2d/2d483c93.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>71d57C76BpL</td>\n      <td>What is the shade shape?</td>\n      <td>Drum</td>\n      <td>b8/b8b510a2.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# final_df= final_df.head(38000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T06:19:32.115593Z","iopub.execute_input":"2025-05-17T06:19:32.115844Z","iopub.status.idle":"2025-05-17T06:19:32.119361Z","shell.execute_reply.started":"2025-05-17T06:19:32.115826Z","shell.execute_reply":"2025-05-17T06:19:32.118682Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"len(final_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T06:19:32.325266Z","iopub.execute_input":"2025-05-17T06:19:32.325472Z","iopub.status.idle":"2025-05-17T06:19:32.330343Z","shell.execute_reply.started":"2025-05-17T06:19:32.325456Z","shell.execute_reply":"2025-05-17T06:19:32.329642Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"63639"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"len(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T06:16:10.637074Z","iopub.execute_input":"2025-05-17T06:16:10.637338Z","iopub.status.idle":"2025-05-17T06:16:10.641778Z","shell.execute_reply.started":"2025-05-17T06:16:10.637318Z","shell.execute_reply":"2025-05-17T06:16:10.641214Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"15910"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"!pip install transformers accelerate timm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ncsv_path = \"/kaggle/input/vr-dataset/images.csv\"\ndf = pd.read_csv(csv_path)\nprint(df.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_id_to_path = dict(zip(df['image_id'], df['path']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\n\nwith open('/kaggle/input/vr-dataset/merge_prepro_t.json', 'r') as f:\n    qa_data = json.load(f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"qa_data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Blip2Processor, Blip2ForConditionalGeneration\nimport torch\nfrom PIL import Image\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load BLIP-2 (flan-t5-xl is a good checkpoint for Q&A tasks)\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\", torch_dtype=torch.float16).to(device)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport numpy as np\n\n# Parameters\nBATCH_SIZE = 8  # You can experiment with 8, 16, or 32 depending on GPU memory\n\nbatched_images = []\nbatched_questions = []\nreferences = []\n\nfor entry in qa_data:\n    image_id = entry[\"image_id\"]\n    image_path = f\"/kaggle/input/vr-dataset/small/small/{image_id_to_path[image_id]}\"\n    image = Image.open(image_path).convert(\"RGB\")\n\n    for qa in entry[\"questions\"]:\n        question = qa[\"question\"]\n        true_answer = qa[\"answer\"]\n\n        batched_images.append(image)\n        batched_questions.append(question)\n        references.append(true_answer)\n\nprint(f\"Total QA pairs: {len(batched_questions)}\")\n\n# Now process in batches\npredictions = []\n\nfor i in range(0, len(batched_questions), BATCH_SIZE):\n    batch_imgs = batched_images[i:i+BATCH_SIZE]\n    batch_qs = batched_questions[i:i+BATCH_SIZE]\n\n    inputs = processor(images=batch_imgs, text=batch_qs, return_tensors=\"pt\", padding=True).to(device, torch.float16)\n    generated_ids = model.generate(**inputs, max_new_tokens=10)\n\n    batch_outputs = processor.batch_decode(generated_ids, skip_special_tokens=True)\n    print(batch_outputs)\n    predictions.extend([ans.strip() for ans in batch_outputs])\n\nprint(\"done batching\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install bert-score \nfrom bert_score import score\nfrom sklearn.metrics import accuracy_score\n\n# BERTScore\nP, R, F1 = score(predictions, references, lang=\"en\", verbose=True)\n\n# Exact match (case-insensitive single-word comparison)\nexact_match = [pred.lower() == ref.lower() for pred, ref in zip(predictions, references)]\n\n# Print results\nprint(f\"Average BERTScore F1: {F1.mean().item():.4f}\")\nprint(f\"Exact Match Accuracy: {sum(exact_match) / len(exact_match):.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LORA - MODEL","metadata":{}},{"cell_type":"code","source":"final_df.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create PyTorch DataSet","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Subset,DataLoader\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\nfrom peft import get_peft_model, LoraConfig, TaskType\nfrom datasets import Dataset,load_dataset\nfrom PIL import Image\nimport pandas as pd\nfrom torchvision import transforms\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = torch.load(\"/kaggle/input/tensor-data/train_batch_2.pt\", weights_only=True)\nprint(\"Train Dataset created....\")\n# train_batch_2 = torch.load(\"/kaggle/input/tensor-data/train_batch_2.pt\", weights_only=True)\ntest_dataset = torch.load(\"/kaggle/input/tensor-data/test_batch.pt\", weights_only=True)\nprint(\"Test Dataset created....\")\n\n# Combine both datasets\n# train_dataset = train_batch_1 + train_batch_2\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_dataset[0])\nprint(len(train_dataset))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(test_dataset[0])\nprint(len(test_dataset))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def collate_fn(batch):\n    pixel_values = torch.stack([x[\"pixel_values\"] for x in batch])\n    input_ids = torch.nn.utils.rnn.pad_sequence(\n        [x[\"input_ids\"] for x in batch], batch_first=True, padding_value=processor.tokenizer.pad_token_id\n    )\n    attention_mask = torch.nn.utils.rnn.pad_sequence(\n        [x[\"attention_mask\"] for x in batch], batch_first=True, padding_value=0\n    )\n    labels = torch.nn.utils.rnn.pad_sequence(\n        [x[\"labels\"] for x in batch], batch_first=True, padding_value=-100\n    )\n    return {\n        \"pixel_values\": pixel_values,\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels\n    }\n\n#try with lower batch size if memory gets full\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn, num_workers=4)\ntest_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn, num_workers=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Blip2ForConditionalGeneration\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\", torch_dtype=torch.float16)\nmodel.to(device)\n# right after you load the model:\n_orig_forward = model.forward\n\ndef _forward_no_inputs_embeds(*args, **kwargs):\n    kwargs.pop(\"inputs_embeds\", None)\n    kwargs.pop(\"decoder_inputs_embeds\", None)  # Use `None` as default\n    return _orig_forward(*args, **kwargs)\n\n\nmodel.forward = _forward_no_inputs_embeds\n\nlora_config = LoraConfig(\n    r=128,  # Rank of the low-rank matrices\n    lora_alpha=256,  # Scaling factor\n    target_modules=[\"q\", \"v\"],  # Target attention layers\n    lora_dropout=0.01,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# Apply LoRA to the model\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()  # Verify the number of trainable parameters","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import get_scheduler\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\nscaler = torch.cuda.amp.GradScaler()  # for mixed precision\nnum_epochs=3\n\n\n# Create the scheduler (assuming num_epochs and train_dataloader are defined)\n# num_train_steps = len(train_dataloader) * num_epochs  # Total training steps\nnum_train_steps = 1000\nwarmup_steps = int(num_train_steps * 0.1)  # Warm-up for 10% of total steps\n\n# Set up the learning rate scheduler\nlr_scheduler = get_scheduler(\n    \"cosine\",  # Linear warm-up followed by linear decay\n    optimizer=optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=num_train_steps\n)\n# new part added\n\nfor epoch in range(num_epochs):\n    model.train()\n    for batch in train_dataloader:\n        # Assume batch is a dict with keys 'pixel_values', 'input_ids', 'attention_mask', 'labels'\n        pixel_values = batch[\"pixel_values\"].to(device, torch.float16)        # image tensor\n        input_ids = batch[\"input_ids\"].to(device)                            # text input IDs\n        attention_mask = batch[\"attention_mask\"].to(device)                  # text mask\n        labels = batch[\"labels\"].to(device)                                  # target IDs (with -100 as needed)\n\n        optimizer.zero_grad()\n        with torch.cuda.amp.autocast():  # mixed precision context\n            outputs = model(\n                pixel_values=pixel_values,\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n            loss = outputs.loss\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        #this one line is the only change\n       # Update the learning rate with the scheduler\n        lr_scheduler.step()  # This updates the learning rate after each batch\n        \n    # Validation loop\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for batch in test_dataloader:\n            pixel_values = batch[\"pixel_values\"].to(device, torch.float16)\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            outputs = model(\n                pixel_values=pixel_values,\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n            val_loss += outputs.loss.item()\n    val_loss /= len(test_dataloader)\n    print(f\"Epoch {epoch}: validation loss = {val_loss}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n!pip install bert-score\nfrom bert_score import score\n\nmodel.eval()\npredictions = []\nreferences = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        pixel_values = batch[\"pixel_values\"].to(device, torch.float16)\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        generated_ids = model.generate(\n            pixel_values=pixel_values,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_new_tokens=20  # Adjust as needed\n        )\n\n        # Decode generated predictions\n        generated_texts = processor.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n        predictions.extend(generated_texts)\n\n        # Decode ground-truth labels\n        label_ids = batch[\"labels\"]\n        label_ids[label_ids == -100] = processor.tokenizer.pad_token_id  # Replace -100 with pad_token_id\n        true_texts = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n        references.extend(true_texts)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"P, R, F1 = score(predictions, references, lang=\"en\", verbose=True)\n\nprint(f\"Average BERTScore F1: {F1.mean().item():.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"references","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LORA MODEL TRAINING ","metadata":{}},{"cell_type":"code","source":"!pip install -q peft accelerate\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BlipProcessor, BlipForQuestionAnswering, get_scheduler\nfrom peft import get_peft_model, LoraConfig, TaskType, PeftModel\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n\nmodel = BlipForQuestionAnswering.from_pretrained(\n    \"Salesforce/blip-vqa-base\"\n).to(device)\n\n_orig_forward = model.forward\n\ndef _forward_no_inputs_embeds(*args, **kwargs):\n    kwargs.pop(\"inputs_embeds\", None)\n    kwargs.pop(\"decoder_inputs_embeds\", None)  # Use `None` as default\n    return _orig_forward(*args, **kwargs)\n\nmodel.forward = _forward_no_inputs_embeds\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"query\", \"value\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\ndf = pd.read_csv(\"/kaggle/input/finale-dataset/train_dataset.csv\")  # assumes columns: 'path', 'question', 'answer'\n\nclass BlipVQADataset(Dataset):\n    def __init__(self, df, image_dir, processor):\n        self.df = df\n        self.image_dir = image_dir\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image_path = os.path.join(self.image_dir, row['path'])\n        image = Image.open(image_path).convert(\"RGB\")\n        question = str(row[\"question\"])\n        answer = str(row[\"answer\"])\n\n        inputs = self.processor(images=image, text=question, return_tensors=\"pt\", padding='max_length', max_length=128, truncation=True)\n        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n        inputs[\"labels\"] = processor.tokenizer(answer, return_tensors=\"pt\", padding='max_length', max_length=32, truncation=True).input_ids.squeeze(0)\n        return inputs\n\ndef collate_fn(batch):\n    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n    labels = torch.nn.utils.rnn.pad_sequence(\n        [item[\"labels\"] for item in batch], batch_first=True, padding_value=-100\n    )\n    return {\n        \"pixel_values\": pixel_values,\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"labels\": labels\n    }\n\nimage_dir = \"/kaggle/input/vr-dataset/small/small/\"\ndataset = BlipVQADataset(df, image_dir, processor)\ndataloader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=collate_fn, num_workers=2)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\nlr_scheduler = get_scheduler(\"cosine\", optimizer=optimizer, num_warmup_steps=100, num_training_steps=len(dataloader)*3)\n\nscaler = torch.cuda.amp.GradScaler()\n\nmodel.train()\nfor epoch in range(3):\n    total_loss = 0\n    for step, batch in enumerate(dataloader):\n        pixel_values = batch[\"pixel_values\"].to(device, dtype=torch.float16)\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        optimizer.zero_grad()\n        with torch.cuda.amp.autocast():\n            # Correct forward pass without 'inputs_embeds'\n            outputs = model(\n                pixel_values=pixel_values,  # Image features\n                input_ids=input_ids,        # Text input (question)\n                attention_mask=attention_mask,  # Attention mask\n                labels=labels               # Labels for the answer\n            )\n            loss = outputs.loss\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n        lr_scheduler.step()\n\n        total_loss += loss.item()\n        if step % 10 == 0:\n            print(f\"Epoch {epoch+1}, Step {step}, Loss: {loss.item():.4f}\")\n\n    print(f\"Epoch {epoch+1} complete. Avg Loss: {total_loss / len(dataloader):.4f}\")\n\nsave_path = \"/kaggle/working/blip_lora_adapter\"\nmodel.save_pretrained(save_path)\nprint(f\"LoRA adapter saved at: {save_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BlipVQATestDataset(Dataset):\n    def __init__(self, df, image_dir, processor):\n        self.df = df\n        self.image_dir = image_dir\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image_path = os.path.join(self.image_dir, row['path'])\n        image = Image.open(image_path).convert(\"RGB\")\n        question = str(row[\"question\"])\n\n        inputs = self.processor(images=image, text=question, return_tensors=\"pt\", padding='max_length', max_length=128, truncation=True)\n        return {k: v.squeeze(0) for k, v in inputs.items()}\ntest_dataset = BlipVQATestDataset(test_df, image_dir=\"/kaggle/input/vr-dataset/small/small/\", processor=processor)\ntest_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\nmodel.eval()\npredictions = []\n\nwith torch.no_grad():\n    for batch in test_dataloader:\n        pixel_values = batch[\"pixel_values\"].to(device)\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        outputs = model.generate(\n            pixel_values=pixel_values,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_new_tokens=5\n        )\n\n        answers = processor.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        predictions.extend(answers)\ntest_df[\"predicted_answer\"] = predictions\ntest_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom bert_score import score\n\ntest_df[\"Answer_clean\"] = test_df[\"answer\"].astype(str).str.lower().str.strip()\ntest_df[\"Pred_clean\"] = test_df[\"predicted_answer\"].astype(str).str.lower().str.strip()\n\nacc = accuracy_score(test_df[\"Answer_clean\"], test_df[\"Pred_clean\"])\nprint(f\"Exact Match Accuracy: {acc:.4f}\")\n\nfiltered_df = test_df[(test_df[\"Answer_clean\"] != \"\") & (test_df[\"Pred_clean\"] != \"\")]\n\nP, R, F1 = score(\n    filtered_df[\"Pred_clean\"].tolist(),\n    filtered_df[\"Answer_clean\"].tolist(),\n    lang=\"en\",\n    verbose=True\n)\nprint(f\"BERTScore - P: {P.mean():.4f}, R: {R.mean():.4f}, F1: {F1.mean():.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_path = \"/kaggle/working/blip_lora_adapter\"\nmodel.save_pretrained(save_path)\nprint(f\"LoRA adapter saved at: {save_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/blip_lora_adapter.zip /kaggle/working/blip_lora_adapter\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Flash","metadata":{}},{"cell_type":"code","source":"!pip install -q peft transformers accelerate timm\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BlipForQuestionAnswering, BlipProcessor\n\nbase_model_name = \"Salesforce/blip-vqa-base\"\nprocessor = BlipProcessor.from_pretrained(base_model_name)\nmodel = BlipForQuestionAnswering.from_pretrained(base_model_name)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import PeftModel\n\nadapter_path = \"/kaggle/input/lora-model/adapter_model.safetensors\"\nmodel = PeftModel.from_pretrained(model, adapter_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# BASE-MODEL","metadata":{}},{"cell_type":"code","source":"!pip install transformers torch torchvision pillow\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:12:17.642379Z","iopub.execute_input":"2025-05-17T07:12:17.642895Z","iopub.status.idle":"2025-05-17T07:12:20.933547Z","shell.execute_reply.started":"2025-05-17T07:12:17.642871Z","shell.execute_reply":"2025-05-17T07:12:20.932705Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\nfrom PIL import Image\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom bert_score import score\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Model & processor\nmodel_name = \"Salesforce/blip-vqa-base\"\nprocessor = BlipProcessor.from_pretrained(model_name)\nmodel = BlipForQuestionAnswering.from_pretrained(model_name).to(device)\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:26:20.823864Z","iopub.execute_input":"2025-05-17T07:26:20.824524Z","iopub.status.idle":"2025-05-17T07:26:22.691691Z","shell.execute_reply.started":"2025-05-17T07:26:20.824498Z","shell.execute_reply":"2025-05-17T07:26:22.691061Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"BlipForQuestionAnswering(\n  (vision_model): BlipVisionModel(\n    (embeddings): BlipVisionEmbeddings(\n      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    )\n    (encoder): BlipEncoder(\n      (layers): ModuleList(\n        (0-11): 12 x BlipEncoderLayer(\n          (self_attn): BlipAttention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n            (projection): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): BlipMLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (text_encoder): BlipTextModel(\n    (embeddings): BlipTextEmbeddings(\n      (word_embeddings): Embedding(30524, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): BlipTextEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BlipTextLayer(\n          (attention): BlipTextAttention(\n            (self): BlipTextSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): BlipTextSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (crossattention): BlipTextAttention(\n            (self): BlipTextSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): BlipTextSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): BlipTextIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BlipTextOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (text_decoder): BlipTextLMHeadModel(\n    (bert): BlipTextModel(\n      (embeddings): BlipTextEmbeddings(\n        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (encoder): BlipTextEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x BlipTextLayer(\n            (attention): BlipTextAttention(\n              (self): BlipTextSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (output): BlipTextSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (crossattention): BlipTextAttention(\n              (self): BlipTextSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (output): BlipTextSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (intermediate): BlipTextIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BlipTextOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n        )\n      )\n    )\n    (cls): BlipTextOnlyMLMHead(\n      (predictions): BlipTextLMPredictionHead(\n        (transform): BlipTextPredictionHeadTransform(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (transform_act_fn): GELUActivation()\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"class VQADataset(Dataset):\n    def __init__(self, df, image_dir, processor):\n        self.df = df.reset_index(drop=True)\n        self.image_dir = image_dir\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.image_dir, row[\"path\"])\n        image = Image.open(img_path).convert(\"RGB\")\n        question = row[\"question\"]\n\n        inputs = self.processor(\n            image,\n            question,\n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            max_length=128,\n            truncation=True\n        )\n        return {\n            \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),\n            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n            \"answer\": row[\"answer\"]\n        }\n\n# Custom collate function\ndef collate_fn(batch):\n    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n    input_ids = torch.stack([item[\"input_ids\"] for item in batch])\n    attention_mask = torch.stack([item[\"attention_mask\"] for item in batch])\n    answers = [item[\"answer\"] for item in batch]\n    return {\n        \"pixel_values\": pixel_values,\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask,\n        \"answer\": answers\n    }\n\n\n\nimage_dir = \"/kaggle/input/vr-dataset/small/small\" \n\n# Prepare DataLoader\ntest_dataset = VQADataset(test_df, image_dir, processor)\ntest_loader = DataLoader(test_dataset, batch_size=8, collate_fn=collate_fn)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:27:07.769852Z","iopub.execute_input":"2025-05-17T07:27:07.770145Z","iopub.status.idle":"2025-05-17T07:27:07.786984Z","shell.execute_reply.started":"2025-05-17T07:27:07.770127Z","shell.execute_reply":"2025-05-17T07:27:07.786280Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"predictions = []\ntrue_answers = []\n\nwith torch.no_grad():\n    for batch in test_loader:\n        pixel_values = batch[\"pixel_values\"].to(device)\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        outputs = model.generate(\n            pixel_values=pixel_values,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_new_tokens=10,\n            use_cache=True\n        )\n\n        preds = processor.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        predictions.extend(preds)\n        true_answers.extend(batch[\"answer\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:27:25.559141Z","iopub.execute_input":"2025-05-17T07:27:25.559405Z","iopub.status.idle":"2025-05-17T07:39:27.247465Z","shell.execute_reply.started":"2025-05-17T07:27:25.559386Z","shell.execute_reply":"2025-05-17T07:39:27.246857Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"!pip install bert-score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:25:56.155954Z","iopub.execute_input":"2025-05-17T07:25:56.156559Z","iopub.status.idle":"2025-05-17T07:25:59.624316Z","shell.execute_reply.started":"2025-05-17T07:25:56.156537Z","shell.execute_reply":"2025-05-17T07:25:59.623582Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting bert-score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.6.0+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.51.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert-score) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert-score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert-score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert-score) (3.7.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert-score) (25.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->bert-score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert-score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert-score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.31.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert-score) (0.5.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert-score) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert-score) (2025.4.26)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers>=3.0.0->bert-score) (1.1.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->bert-score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->bert-score) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->bert-score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->bert-score) (2024.2.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bert-score\nSuccessfully installed bert-score-0.3.13\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"test_df[\"predicted_answer\"] = predictions\ntest_df[\"Answer_clean\"] = test_df[\"answer\"].astype(str).str.lower().str.strip()\ntest_df[\"Pred_clean\"] = test_df[\"predicted_answer\"].astype(str).str.lower().str.strip()\n\nacc = accuracy_score(test_df[\"Answer_clean\"], test_df[\"Pred_clean\"])\nprint(f\"Exact Match Accuracy: {acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:39:27.248660Z","iopub.execute_input":"2025-05-17T07:39:27.248919Z","iopub.status.idle":"2025-05-17T07:39:27.290322Z","shell.execute_reply.started":"2025-05-17T07:39:27.248895Z","shell.execute_reply":"2025-05-17T07:39:27.289767Z"}},"outputs":[{"name":"stdout","text":"Exact Match Accuracy: 0.1725\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"filtered_df = test_df[(test_df[\"Answer_clean\"] != \"\") & (test_df[\"Pred_clean\"] != \"\")]\n\nP, R, F1 = score(\n    filtered_df[\"Pred_clean\"].tolist(),\n    filtered_df[\"Answer_clean\"].tolist(),\n    lang=\"en\",\n    verbose=True\n)\n\nprint(f\"BERTScore - Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:39:27.290939Z","iopub.execute_input":"2025-05-17T07:39:27.291206Z","iopub.status.idle":"2025-05-17T07:39:46.794921Z","shell.execute_reply.started":"2025-05-17T07:39:27.291180Z","shell.execute_reply":"2025-05-17T07:39:46.793898Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f96f289f53c44a63a7e715155a794d6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bd8c9ab19e247cf9e9d88f1fae6189d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1de0f231d29c4c5bbc8f1088257271b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31e58923750649e28a8782ead564e503"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"065c67ebbe1c4704a8ae7cb4e827766f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d21f727c55b0483099a59f313101ddf9"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/40 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9cb207eaa884f18a9ef9fa1035b01b9"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/249 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66a533bc184141f99d679820a3a88ec7"}},"metadata":{}},{"name":"stdout","text":"done in 4.73 seconds, 3363.31 sentences/sec\nBERTScore - Precision: 0.9517, Recall: 0.9251, F1: 0.9371\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"import nltk\nnltk.download(\"wordnet\")\nnltk.download(\"omw-1.4\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:43:06.943132Z","iopub.execute_input":"2025-05-17T07:43:06.943422Z","iopub.status.idle":"2025-05-17T07:43:07.147151Z","shell.execute_reply.started":"2025-05-17T07:43:06.943401Z","shell.execute_reply":"2025-05-17T07:43:07.146513Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n[nltk_data]   Package omw-1.4 is already up-to-date!\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"from nltk.corpus import wordnet as wn\n\ndef wups_score(preds, refs, threshold=0.9):\n    def compute_wups(pred, ref):\n        pred_synsets = wn.synsets(pred)\n        ref_synsets = wn.synsets(ref)\n\n        if not pred_synsets or not ref_synsets:\n            return 0.0  # no synsets found  assume unrelated\n\n        max_score = max(wn.wup_similarity(p, r) or 0.0 for p in pred_synsets for r in ref_synsets)\n        return max_score\n\n    scores = []\n    for pred, ref in zip(preds, refs):\n        pred = pred.lower().strip()\n        ref = ref.lower().strip()\n\n        score = compute_wups(pred, ref)\n        score = score if score >= threshold else 0.0  # apply threshold\n        scores.append(score)\n\n    return sum(scores) / len(scores) if scores else 0.0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:43:20.087967Z","iopub.execute_input":"2025-05-17T07:43:20.088462Z","iopub.status.idle":"2025-05-17T07:43:20.094402Z","shell.execute_reply.started":"2025-05-17T07:43:20.088440Z","shell.execute_reply":"2025-05-17T07:43:20.093614Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"test_df[\"Answer_clean\"] = test_df[\"answer\"].astype(str).str.lower().str.strip()\ntest_df[\"Pred_clean\"] = test_df[\"predicted_answer\"].astype(str).str.lower().str.strip()\n\nrefs = test_df[\"Answer_clean\"].tolist()\npreds = test_df[\"Pred_clean\"].tolist()\n\nwups_00 = wups_score(preds, refs, threshold=0.0)\nwups_09 = wups_score(preds, refs, threshold=0.9)\n\nfinal_score = 0.5 * wups_00 + 0.5 * wups_09\n\nprint(f\"WUPS @0.0 (lenient): {wups_00:.4f}\")\nprint(f\"WUPS @0.9 (strict): {wups_09:.4f}\")\nprint(f\"Final Weighted WUPS Score: {final_score:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:44:22.193244Z","iopub.execute_input":"2025-05-17T07:44:22.193772Z","iopub.status.idle":"2025-05-17T07:46:09.561013Z","shell.execute_reply.started":"2025-05-17T07:44:22.193731Z","shell.execute_reply":"2025-05-17T07:46:09.560332Z"}},"outputs":[{"name":"stdout","text":"WUPS @0.0 (lenient): 0.4538\nWUPS @0.9 (strict): 0.2116\nFinal Weighted WUPS Score: 0.3327\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, util\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight & fast\n\npreds = test_df[\"predicted_answer\"].astype(str).tolist()\nrefs = test_df[\"answer\"].astype(str).tolist()\n\npred_embeds = model.encode(preds, convert_to_tensor=True)\nref_embeds = model.encode(refs, convert_to_tensor=True)\n\ncos_sim = util.cos_sim(pred_embeds, ref_embeds).diagonal()\n\nsbert_score = cos_sim.mean().item()\nprint(f\"SBERT Metric: {sbert_score:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:46:27.383527Z","iopub.execute_input":"2025-05-17T07:46:27.383862Z","iopub.status.idle":"2025-05-17T07:46:34.478340Z","shell.execute_reply.started":"2025-05-17T07:46:27.383838Z","shell.execute_reply":"2025-05-17T07:46:34.477653Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/498 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7bd34f45ba54a928e747432b4faa531"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/498 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c623860cebf49b487525156321cbd9e"}},"metadata":{}},{"name":"stdout","text":"SBERT Metric: 0.4584\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"import nltk\nnltk.download('wordnet')\nnltk.download('punkt')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:46:34.479710Z","iopub.execute_input":"2025-05-17T07:46:34.479979Z","iopub.status.idle":"2025-05-17T07:46:34.515361Z","shell.execute_reply.started":"2025-05-17T07:46:34.479962Z","shell.execute_reply":"2025-05-17T07:46:34.514638Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"from nltk.translate.meteor_score import meteor_score\nfrom nltk.tokenize import word_tokenize\n\n# Tokenize and compute METEOR per pair\nmeteor_scores = [\n    meteor_score([word_tokenize(ref)], word_tokenize(pred))\n    for ref, pred in zip(test_df[\"answer\"].astype(str), test_df[\"predicted_answer\"].astype(str))\n]\n\n# Average score\navg_meteor = sum(meteor_scores) / len(meteor_scores)\nprint(f\"Average METEOR Score: {avg_meteor:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:46:34.516197Z","iopub.execute_input":"2025-05-17T07:46:34.516599Z","iopub.status.idle":"2025-05-17T07:46:36.601889Z","shell.execute_reply.started":"2025-05-17T07:46:34.516580Z","shell.execute_reply":"2025-05-17T07:46:36.601151Z"}},"outputs":[{"name":"stdout","text":"Average METEOR Score: 0.0994\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:40:28.435773Z","iopub.execute_input":"2025-05-17T07:40:28.436044Z","iopub.status.idle":"2025-05-17T07:40:28.446504Z","shell.execute_reply.started":"2025-05-17T07:40:28.436026Z","shell.execute_reply":"2025-05-17T07:40:28.445657Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"      image_id                             question    answer  \\\n0  71OQ4BiD3uL            What is the product type?    Bucket   \n1  81CtH3N+0NL   What kind of closure does it have?   Lace-up   \n2  8176aHTRP3L           What is the primary color?     Multi   \n3  71yC+5KLN-L  What material is the cover made of?      Hard   \n4  71pP-5SEtyL   What material is the case made of?  Silicone   \n\n              path predicted_answer Answer_clean Pred_clean  \n0  f0/f0d50e0a.jpg               no       bucket         no  \n1  52/521668c5.jpg          sliding      lace-up    sliding  \n2  43/438d87ff.jpg             blue        multi       blue  \n3  30/304914ac.jpg            metal         hard      metal  \n4  40/40d177f8.jpg            metal     silicone      metal  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>question</th>\n      <th>answer</th>\n      <th>path</th>\n      <th>predicted_answer</th>\n      <th>Answer_clean</th>\n      <th>Pred_clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>71OQ4BiD3uL</td>\n      <td>What is the product type?</td>\n      <td>Bucket</td>\n      <td>f0/f0d50e0a.jpg</td>\n      <td>no</td>\n      <td>bucket</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>81CtH3N+0NL</td>\n      <td>What kind of closure does it have?</td>\n      <td>Lace-up</td>\n      <td>52/521668c5.jpg</td>\n      <td>sliding</td>\n      <td>lace-up</td>\n      <td>sliding</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8176aHTRP3L</td>\n      <td>What is the primary color?</td>\n      <td>Multi</td>\n      <td>43/438d87ff.jpg</td>\n      <td>blue</td>\n      <td>multi</td>\n      <td>blue</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>71yC+5KLN-L</td>\n      <td>What material is the cover made of?</td>\n      <td>Hard</td>\n      <td>30/304914ac.jpg</td>\n      <td>metal</td>\n      <td>hard</td>\n      <td>metal</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>71pP-5SEtyL</td>\n      <td>What material is the case made of?</td>\n      <td>Silicone</td>\n      <td>40/40d177f8.jpg</td>\n      <td>metal</td>\n      <td>silicone</td>\n      <td>metal</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":36},{"cell_type":"markdown","source":"# MODEL-2","metadata":{}},{"cell_type":"code","source":"!pip install torch torchvision transformers peft pandas pillow\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T06:19:01.676223Z","iopub.execute_input":"2025-05-17T06:19:01.676861Z","iopub.status.idle":"2025-05-17T06:19:04.834915Z","shell.execute_reply.started":"2025-05-17T06:19:01.676837Z","shell.execute_reply":"2025-05-17T06:19:04.834247Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import BlipProcessor, BlipForQuestionAnswering\nfrom peft import PeftModel\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprocessor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n\nmodel = BlipForQuestionAnswering.from_pretrained(\n    \"Salesforce/blip-vqa-base\",\n    device_map='auto'\n)\n\n\nmodel = PeftModel.from_pretrained(model, \"bk45/blip-vqa-finetuned\").to(device)\nmodel.eval()\n\n# Dataset class\nclass BlipVQATestDataset(Dataset):\n    def __init__(self, df, image_dir, processor):\n        self.df = df\n        self.image_dir = image_dir\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image_path = os.path.join(self.image_dir, row['path'])\n        image = Image.open(image_path).convert(\"RGB\")\n        question = str(row[\"question\"])\n\n        inputs = self.processor(\n            images=image,\n            text=question,\n            return_tensors=\"pt\",\n            padding='max_length',\n            max_length=128,\n            truncation=True\n        )\n        return {k: v.squeeze(0) for k, v in inputs.items()}\n\n# Load test dataframe & image directory\n# test_df = pd.read_csv(\"/kaggle/input/vr-dataset/test.csv\")  # Adjust path if needed\nimage_dir = \"/kaggle/input/vr-dataset/small/small/\"\n\n# Prepare DataLoader\ntest_dataset = BlipVQATestDataset(test_df, image_dir, processor)\ntest_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n\n# Inference loop\npredictions = []\nwith torch.no_grad():\n    for batch in test_dataloader:\n        pixel_values = batch[\"pixel_values\"].to(device)\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        outputs = model.generate(\n            pixel_values=pixel_values,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_new_tokens=10,\n            use_cache=True\n        )\n\n        answers = processor.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        predictions.extend(answers)\n\n# Save or print predictions\ntest_df[\"predicted_answer\"] = predictions\ntest_df.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T06:19:40.380974Z","iopub.execute_input":"2025-05-17T06:19:40.381250Z","iopub.status.idle":"2025-05-17T06:29:54.824371Z","shell.execute_reply.started":"2025-05-17T06:19:40.381232Z","shell.execute_reply":"2025-05-17T06:29:54.823480Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"      image_id                             question    answer  \\\n0  71OQ4BiD3uL            What is the product type?    Bucket   \n1  81CtH3N+0NL   What kind of closure does it have?   Lace-up   \n2  8176aHTRP3L           What is the primary color?     Multi   \n3  71yC+5KLN-L  What material is the cover made of?      Hard   \n4  71pP-5SEtyL   What material is the case made of?  Silicone   \n5  71A8O6XIQGL            What is the shoe's style?    Ballet   \n6  61NxifiMbuL                Is warranty included?        No   \n7  71sqlEJ6F1L         What is the case's material?   Silicon   \n8  81UrYa7KFEL              What is the heat level?    Medium   \n9  71gGW9pDoHL            What is the shoe's style?  Sneakers   \n\n              path predicted_answer  \n0  f0/f0d50e0a.jpg             hand  \n1  52/521668c5.jpg             lace  \n2  43/438d87ff.jpg             pink  \n3  30/304914ac.jpg             hard  \n4  40/40d177f8.jpg           canvas  \n5  10/104a3774.jpg            heels  \n6  42/4213f453.jpg               no  \n7  3c/3ce01595.jpg           canvas  \n8  b6/b613a781.jpg           medium  \n9  e2/e2f5d987.jpg         sneakers  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>question</th>\n      <th>answer</th>\n      <th>path</th>\n      <th>predicted_answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>71OQ4BiD3uL</td>\n      <td>What is the product type?</td>\n      <td>Bucket</td>\n      <td>f0/f0d50e0a.jpg</td>\n      <td>hand</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>81CtH3N+0NL</td>\n      <td>What kind of closure does it have?</td>\n      <td>Lace-up</td>\n      <td>52/521668c5.jpg</td>\n      <td>lace</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8176aHTRP3L</td>\n      <td>What is the primary color?</td>\n      <td>Multi</td>\n      <td>43/438d87ff.jpg</td>\n      <td>pink</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>71yC+5KLN-L</td>\n      <td>What material is the cover made of?</td>\n      <td>Hard</td>\n      <td>30/304914ac.jpg</td>\n      <td>hard</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>71pP-5SEtyL</td>\n      <td>What material is the case made of?</td>\n      <td>Silicone</td>\n      <td>40/40d177f8.jpg</td>\n      <td>canvas</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>71A8O6XIQGL</td>\n      <td>What is the shoe's style?</td>\n      <td>Ballet</td>\n      <td>10/104a3774.jpg</td>\n      <td>heels</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>61NxifiMbuL</td>\n      <td>Is warranty included?</td>\n      <td>No</td>\n      <td>42/4213f453.jpg</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>71sqlEJ6F1L</td>\n      <td>What is the case's material?</td>\n      <td>Silicon</td>\n      <td>3c/3ce01595.jpg</td>\n      <td>canvas</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>81UrYa7KFEL</td>\n      <td>What is the heat level?</td>\n      <td>Medium</td>\n      <td>b6/b613a781.jpg</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>71gGW9pDoHL</td>\n      <td>What is the shoe's style?</td>\n      <td>Sneakers</td>\n      <td>e2/e2f5d987.jpg</td>\n      <td>sneakers</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"test_df.to_csv(\"vqa_predictions.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T06:31:31.919849Z","iopub.execute_input":"2025-05-17T06:31:31.920178Z","iopub.status.idle":"2025-05-17T06:31:31.975378Z","shell.execute_reply.started":"2025-05-17T06:31:31.920152Z","shell.execute_reply":"2025-05-17T06:31:31.974882Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"test_df = pd.read_csv(\"vqa_predictions.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:49:33.041537Z","iopub.execute_input":"2025-05-17T07:49:33.041861Z","iopub.status.idle":"2025-05-17T07:49:33.069393Z","shell.execute_reply.started":"2025-05-17T07:49:33.041841Z","shell.execute_reply":"2025-05-17T07:49:33.068671Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:49:33.530998Z","iopub.execute_input":"2025-05-17T07:49:33.531524Z","iopub.status.idle":"2025-05-17T07:49:33.539223Z","shell.execute_reply.started":"2025-05-17T07:49:33.531501Z","shell.execute_reply":"2025-05-17T07:49:33.538479Z"}},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"      image_id                             question    answer  \\\n0  71OQ4BiD3uL            What is the product type?    Bucket   \n1  81CtH3N+0NL   What kind of closure does it have?   Lace-up   \n2  8176aHTRP3L           What is the primary color?     Multi   \n3  71yC+5KLN-L  What material is the cover made of?      Hard   \n4  71pP-5SEtyL   What material is the case made of?  Silicone   \n\n              path predicted_answer  \n0  f0/f0d50e0a.jpg             hand  \n1  52/521668c5.jpg             lace  \n2  43/438d87ff.jpg             pink  \n3  30/304914ac.jpg             hard  \n4  40/40d177f8.jpg           canvas  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_id</th>\n      <th>question</th>\n      <th>answer</th>\n      <th>path</th>\n      <th>predicted_answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>71OQ4BiD3uL</td>\n      <td>What is the product type?</td>\n      <td>Bucket</td>\n      <td>f0/f0d50e0a.jpg</td>\n      <td>hand</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>81CtH3N+0NL</td>\n      <td>What kind of closure does it have?</td>\n      <td>Lace-up</td>\n      <td>52/521668c5.jpg</td>\n      <td>lace</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8176aHTRP3L</td>\n      <td>What is the primary color?</td>\n      <td>Multi</td>\n      <td>43/438d87ff.jpg</td>\n      <td>pink</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>71yC+5KLN-L</td>\n      <td>What material is the cover made of?</td>\n      <td>Hard</td>\n      <td>30/304914ac.jpg</td>\n      <td>hard</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>71pP-5SEtyL</td>\n      <td>What material is the case made of?</td>\n      <td>Silicone</td>\n      <td>40/40d177f8.jpg</td>\n      <td>canvas</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"test_df[\"Answer_clean\"] = test_df[\"answer\"].astype(str).str.lower().str.strip()\ntest_df[\"Pred_clean\"] = test_df[\"predicted_answer\"].astype(str).str.lower().str.strip()\n\nacc = accuracy_score(test_df[\"Answer_clean\"], test_df[\"Pred_clean\"])\nprint(f\"Exact Match Accuracy: {acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:49:58.326087Z","iopub.execute_input":"2025-05-17T07:49:58.326652Z","iopub.status.idle":"2025-05-17T07:49:58.363463Z","shell.execute_reply.started":"2025-05-17T07:49:58.326629Z","shell.execute_reply":"2025-05-17T07:49:58.362900Z"}},"outputs":[{"name":"stdout","text":"Exact Match Accuracy: 0.2864\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"filtered_df = test_df[(test_df[\"Answer_clean\"] != \"\") & (test_df[\"Pred_clean\"] != \"\")]\n\nP, R, F1 = score(\n    filtered_df[\"Pred_clean\"].tolist(),\n    filtered_df[\"Answer_clean\"].tolist(),\n    lang=\"en\",\n    verbose=True\n)\n\nprint(f\"BERTScore - Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:50:28.278148Z","iopub.execute_input":"2025-05-17T07:50:28.278719Z","iopub.status.idle":"2025-05-17T07:50:33.586758Z","shell.execute_reply.started":"2025-05-17T07:50:28.278698Z","shell.execute_reply":"2025-05-17T07:50:33.586181Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"calculating scores...\ncomputing bert embedding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e6ac8d64e4b4321bcff11a7c53e168d"}},"metadata":{}},{"name":"stdout","text":"computing greedy matching.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/249 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9354b3eaad0442e9b31c24dedea42cb8"}},"metadata":{}},{"name":"stdout","text":"done in 4.40 seconds, 3618.49 sentences/sec\nBERTScore - Precision: 0.9552, Recall: 0.9344, F1: 0.9438\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"! pip install nltk\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T06:35:56.571984Z","iopub.execute_input":"2025-05-17T06:35:56.572510Z","iopub.status.idle":"2025-05-17T06:35:59.452956Z","shell.execute_reply.started":"2025-05-17T06:35:56.572489Z","shell.execute_reply":"2025-05-17T06:35:59.451964Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import nltk\nnltk.download(\"wordnet\")\nnltk.download(\"omw-1.4\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T06:36:11.044419Z","iopub.execute_input":"2025-05-17T06:36:11.045072Z","iopub.status.idle":"2025-05-17T06:36:11.880780Z","shell.execute_reply.started":"2025-05-17T06:36:11.045043Z","shell.execute_reply":"2025-05-17T06:36:11.880185Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from nltk.corpus import wordnet as wn\n\ndef wups_score(preds, refs, threshold=0.9):\n    def compute_wups(pred, ref):\n        pred_synsets = wn.synsets(pred)\n        ref_synsets = wn.synsets(ref)\n\n        if not pred_synsets or not ref_synsets:\n            return 0.0  # no synsets found  assume unrelated\n\n        max_score = max(wn.wup_similarity(p, r) or 0.0 for p in pred_synsets for r in ref_synsets)\n        return max_score\n\n    scores = []\n    for pred, ref in zip(preds, refs):\n        pred = pred.lower().strip()\n        ref = ref.lower().strip()\n\n        score = compute_wups(pred, ref)\n        score = score if score >= threshold else 0.0  # apply threshold\n        scores.append(score)\n\n    return sum(scores) / len(scores) if scores else 0.0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T06:36:24.259897Z","iopub.execute_input":"2025-05-17T06:36:24.260526Z","iopub.status.idle":"2025-05-17T06:36:24.266362Z","shell.execute_reply.started":"2025-05-17T06:36:24.260506Z","shell.execute_reply":"2025-05-17T06:36:24.265544Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Clean answers\ntest_df[\"Answer_clean\"] = test_df[\"answer\"].astype(str).str.lower().str.strip()\ntest_df[\"Pred_clean\"] = test_df[\"predicted_answer\"].astype(str).str.lower().str.strip()\n\n# Get lists\nrefs = test_df[\"Answer_clean\"].tolist()\npreds = test_df[\"Pred_clean\"].tolist()\n\n# Calculate WUPS@0.0 (lenient) and WUPS@0.9 (strict)\nwups_00 = wups_score(preds, refs, threshold=0.0)\nwups_09 = wups_score(preds, refs, threshold=0.9)\n\n# Optional: combine\nfinal_score = 0.5 * wups_00 + 0.5 * wups_09\n\n# Print results\nprint(f\"WUPS@0.0 (lenient): {wups_00:.4f}\")\nprint(f\"WUPS@0.9 (strict): {wups_09:.4f}\")\nprint(f\"Final Weighted WUPS Score: {final_score:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T06:40:13.869201Z","iopub.execute_input":"2025-05-17T06:40:13.869496Z","iopub.status.idle":"2025-05-17T06:43:16.565929Z","shell.execute_reply.started":"2025-05-17T06:40:13.869476Z","shell.execute_reply":"2025-05-17T06:43:16.565209Z"}},"outputs":[{"name":"stdout","text":"WUPS@0.0 (lenient): 0.5673\nWUPS@0.9 (strict): 0.3064\nFinal Weighted WUPS Score: 0.4369\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, util\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight & fast\n\npreds = test_df[\"predicted_answer\"].astype(str).tolist()\nrefs = test_df[\"answer\"].astype(str).tolist()\n\npred_embeds = model.encode(preds, convert_to_tensor=True)\nref_embeds = model.encode(refs, convert_to_tensor=True)\n\ncos_sim = util.cos_sim(pred_embeds, ref_embeds).diagonal()\n\nsbert_score = cos_sim.mean().item()\nprint(f\"SBERT Metric: {sbert_score:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:06:00.989889Z","iopub.execute_input":"2025-05-17T07:06:00.990737Z","iopub.status.idle":"2025-05-17T07:06:07.459920Z","shell.execute_reply.started":"2025-05-17T07:06:00.990710Z","shell.execute_reply":"2025-05-17T07:06:07.459290Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/498 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b9c2d4c92834dcea4f06bcba06ab997"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/498 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e532b1de625246d896769681ca469bfe"}},"metadata":{}},{"name":"stdout","text":"SBERT Metric: 0.5630\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"!pip install nltk\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:07:23.350841Z","iopub.execute_input":"2025-05-17T07:07:23.351136Z","iopub.status.idle":"2025-05-17T07:07:26.313504Z","shell.execute_reply.started":"2025-05-17T07:07:23.351116Z","shell.execute_reply":"2025-05-17T07:07:26.312808Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import nltk\nnltk.download('wordnet')\nnltk.download('punkt')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:07:48.934833Z","iopub.execute_input":"2025-05-17T07:07:48.935513Z","iopub.status.idle":"2025-05-17T07:07:49.036554Z","shell.execute_reply.started":"2025-05-17T07:07:48.935487Z","shell.execute_reply":"2025-05-17T07:07:49.035936Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"from nltk.translate.meteor_score import meteor_score\nfrom nltk.tokenize import word_tokenize\n\n# Tokenize and compute METEOR per pair\nmeteor_scores = [\n    meteor_score([word_tokenize(ref)], word_tokenize(pred))\n    for ref, pred in zip(test_df[\"answer\"].astype(str), test_df[\"predicted_answer\"].astype(str))\n]\n\n# Average score\navg_meteor = sum(meteor_scores) / len(meteor_scores)\nprint(f\"Average METEOR Score: {avg_meteor:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T07:09:03.023719Z","iopub.execute_input":"2025-05-17T07:09:03.024458Z","iopub.status.idle":"2025-05-17T07:09:05.058374Z","shell.execute_reply.started":"2025-05-17T07:09:03.024431Z","shell.execute_reply":"2025-05-17T07:09:05.057774Z"}},"outputs":[{"name":"stdout","text":"Average METEOR Score: 0.1496\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}